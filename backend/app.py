import os
import streamlit as st
import librosa
import soundfile as sf
import numpy as np
from transformers import AutoProcessor, AutoModelForAudioClassification
import torch
import torch.nn.functional as F

# Diret√≥rio do modelo treinado
MODEL_DIR = "emotion-recognition-model"

# Verificar se os arquivos necess√°rios existem
if not os.path.exists(MODEL_DIR):
    st.error("O diret√≥rio do modelo treinado n√£o foi encontrado. Verifique a estrutura do projeto.")
    st.stop()

# Carregar o modelo e o processador treinados a partir do diret√≥rio
try:
    processor = AutoProcessor.from_pretrained(MODEL_DIR)
    model = AutoModelForAudioClassification.from_pretrained(MODEL_DIR)
except Exception as e:
    st.error(f"Erro ao carregar o modelo treinado: {e}")
    st.stop()

# Configura√ß√£o da p√°gina do Streamlit
st.set_page_config(page_title="An√°lise de Emo√ß√µes por √Åudio", layout="centered")
st.title("üéôÔ∏è An√°lise de Emo√ß√µes por √Åudio")
st.markdown("Grave um √°udio diretamente no navegador ou envie um arquivo `.wav` para analisar as emo√ß√µes.")

# Upload do √°udio
audio_file = st.file_uploader("Fa√ßa upload de um arquivo de √°udio (.wav)", type=["wav"])

# Exibir bot√£o para grava√ß√£o futura
if st.button("Gravar √Åudio"):
    st.warning("Funcionalidade de grava√ß√£o direto no navegador ainda em desenvolvimento para Streamlit.")

# Processamento do √°udio
if audio_file is not None:
    try:
        # Salvar o √°udio enviado no diret√≥rio apropriado
        UPLOAD_DIR = "uploaded_audio"
        os.makedirs(UPLOAD_DIR, exist_ok=True)
        uploaded_audio_path = os.path.join(UPLOAD_DIR, "uploaded_audio.wav")
        with open(uploaded_audio_path, "wb") as f:
            f.write(audio_file.read())

        # Carregar o √°udio e validar a taxa de amostragem
        audio, sample_rate = librosa.load(uploaded_audio_path, sr=None)
        if sample_rate != 16000:
            st.warning("Ajustando a taxa de amostragem para 16kHz...")
            audio = librosa.resample(audio, orig_sr=sample_rate, target_sr=16000)

        # Processar o √°udio com o modelo
        with st.spinner("Processando o √°udio..."):
            inputs = processor(audio, sampling_rate=16000, return_tensors="pt", padding=True)
            with torch.no_grad():
                logits = model(**inputs).logits

            # Aplicar softmax para calcular as probabilidades
            probs = F.softmax(logits, dim=-1)

            # Identificar a emo√ß√£o
            predicted_id = torch.argmax(probs, dim=-1).item()
            confidence = probs[0, predicted_id].item()
            emotion_labels = ["angry", "disgusted", "happy", "fearful", "neutral", "sad", "surprised"]
            predicted_emotion = emotion_labels[predicted_id]

        # Exibir os resultados ao usu√°rio
        st.success(f"Emo√ß√£o Prevista: **{predicted_emotion}**")
        st.info(f"Confian√ßa: **{confidence:.2f}**")

    except Exception as e:
        st.error(f"Ocorreu um erro ao processar o √°udio: {e}")

# Instru√ß√µes adicionais
st.markdown(
    """
    ### Instru√ß√µes para Grava√ß√£o de √Åudio:
    - Use ferramentas externas para gravar √°udio no formato `.wav` (16kHz).
    - Certifique-se de que o √°udio esteja claro e sem ru√≠dos excessivos para melhores resultados.
    """
)

# Diret√≥rio onde os arquivos ser√£o salvos
MODEL_DIR = "emotion-recognition-model"

# Criar o diret√≥rio caso ele n√£o exista
import os
os.makedirs(MODEL_DIR, exist_ok=True)

# Salvar o modelo e o processador no diret√≥rio
try:
    processor.save_pretrained(MODEL_DIR)
    model.save_pretrained(MODEL_DIR)
    print(f"Arquivos do modelo salvos em {MODEL_DIR}")
except Exception as e:
    print(f"Erro ao salvar os arquivos: {e}")
